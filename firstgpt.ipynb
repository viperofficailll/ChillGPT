{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f731006b-1a59-4c33-b92e-8689fa4c65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tenacity\n",
    "!pip install tenacity==8.1.0\n",
    "\n",
    "!pip install crewai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70a9aedb-863d-45e4-b8de-7a9aabd449f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent ,Task,Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5322171-b2c8-4b52-9832-b19c21e4797f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: utils in c:\\users\\shara\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install utils\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_MODEL_NAME\"] = \"gpt-3.5-turbo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec737f13-94fc-4651-85d1-c1a1271d7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "Planner = Agent(\n",
    "    role=\"Content Planner\",\n",
    "    goal=\"Plan engaging and factually accurate content on {topic}.\",\n",
    "    backstory=(\n",
    "        \"You're working on planning a blog article about the topic: {topic}. \"\n",
    "        \"You collect the information that helps the audience learn something \"\n",
    "        \"and make informed decisions. Your work is the basis for the Content Writer \"\n",
    "        \"to write an article on this topic.\"\n",
    "    ),\n",
    "    allow_delegation=False,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "521ecd02-1d7f-49e2-a5ce-2801d575d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Writer = Agent(\n",
    "    role=\"Content Writer\",\n",
    "    goal=(\n",
    "        \"Write insightful and factually accurate \"\n",
    "        \"opinion piece about the topic: {topic}\"\n",
    "    ),\n",
    "    backstory=(\n",
    "        \"You're working on writing \"\n",
    "        \"a new opinion piece about the topic: {topic}. \"\n",
    "        \"You follow the main objectives and direction of the outline, \"\n",
    "        \"as provided by the Content Planner. \"\n",
    "        \"You also provide objective and impartial insights \"\n",
    "        \"and back them up with the information \"\n",
    "        \"provided by the Content Planner. \"\n",
    "        \"You acknowledge in your opinion piece \"\n",
    "        \"when your statements are opinions as opposed to objective statements.\"\n",
    "    ),\n",
    "    allow_delegation=False,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "099f527e-f27c-4a2f-9adf-f9ff22091830",
   "metadata": {},
   "outputs": [],
   "source": [
    "Editor = Agent(\n",
    "    role=\"Content Editor\",\n",
    "    goal=(\n",
    "        \"Edit and refine the opinion piece written by the Content Writer \"\n",
    "        \"to ensure it is clear, concise, and engaging while maintaining factual accuracy.\"\n",
    "    ),\n",
    "    backstory=(\n",
    "        \"You are tasked with reviewing the opinion piece about the topic: {topic}. \"\n",
    "        \"Your role is to improve the structure, flow, and readability of the content. \"\n",
    "        \"You ensure grammar, spelling, and punctuation are correct, and the tone is consistent. \"\n",
    "        \"You also verify that all statements are supported by evidence and distinguish \"\n",
    "        \"opinions from objective facts. \"\n",
    "        \"Your edits respect the direction and objectives provided by the Content Planner.\"\n",
    "    ),\n",
    "    allow_delegation=False,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2ac384f-cd43-42da-ad6a-a6571063e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = Task(\n",
    "    description=(\n",
    "        \"1. Prioritize the latest trends, key players, and noteworthy news on {topic}.\\n\"\n",
    "        \"2. Identify the target audience, considering their interests and pain points.\\n\"\n",
    "        \"3. Develop a detailed content outline including an introduction, key points, and a call to action.\\n\"\n",
    "        \"4. Include SEO keywords and relevant data or sources.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A comprehensive content plan document with an outline, audience analysis, SEO keywords, \"\n",
    "        \"and resources.\"\n",
    "    ),\n",
    "    agent=Planner\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d10165be-acbc-4a66-8f85-db33f44c7260",
   "metadata": {},
   "outputs": [],
   "source": [
    "write =Task(\n",
    "    description=(\n",
    "        \"1. Use the content plan to craft a compeling blog post on {topic}.\\n\"\n",
    "        \"2.Incorporate SEO keywords naturally.\\n\"\n",
    "        \"3. Sections/Subtitiles are properly named in an engaging manner.\\n\"\n",
    "       \" 4. Ensure the post is structured with an engaging introduction ,insightful body, and a summarizing conclusion .\\n\"\n",
    "       \" 5.Proofread for grammatical errors and alignment with brand's voice.\\n\"\n",
    "    ),\n",
    "    expected_output=\"A well-written blog post \"\n",
    "    \"in markdown format ,ready for publication \"\n",
    "    \"each section should have 2 or 3 paragraphs\",\n",
    "    agent =Writer\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37e606d6-acba-4025-9012-6ef2c904edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit = Task(\n",
    "    description=\"Proofread the given blog post for grammatical errors and alignment with the brand's voice.\",\n",
    "    expected_output=\"A well-written blog post in markdown format, ready for publication. Each section should have 2 or 3 paragraphs.\",\n",
    "    agent=Editor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce3b3ebf-a380-4489-a4b2-65adfa5f3b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crew = Crew(\n",
    "    agents=[Planner, Writer, Editor],\n",
    "    tasks=[plan, write, edit],\n",
    "    verbose=True  # or verbose=False, depending on the desired verbosity level\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d0172ee-69c1-49df-9ca4-a62883a2258a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mContent Planner\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m1. Prioritize the latest trends, key players, and noteworthy news on Artificial Intelligence.\n",
      "2. Identify the target audience, considering their interests and pain points.\n",
      "3. Develop a detailed content outline including an introduction, key points, and a call to action.\n",
      "4. Include SEO keywords and relevant data or sources.\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:LiteLLM call failed: litellm.RateLimitError: RateLimitError: OpenAIException - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mContent Planner\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m1. Prioritize the latest trends, key players, and noteworthy news on Artificial Intelligence.\n",
      "2. Identify the target audience, considering their interests and pain points.\n",
      "3. Develop a detailed content outline including an introduction, key points, and a call to action.\n",
      "4. Include SEO keywords and relevant data or sources.\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:LiteLLM call failed: litellm.RateLimitError: RateLimitError: OpenAIException - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mContent Planner\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m1. Prioritize the latest trends, key players, and noteworthy news on Artificial Intelligence.\n",
      "2. Identify the target audience, considering their interests and pain points.\n",
      "3. Develop a detailed content outline including an introduction, key points, and a call to action.\n",
      "4. Include SEO keywords and relevant data or sources.\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:LiteLLM call failed: litellm.RateLimitError: RateLimitError: OpenAIException - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "litellm.RateLimitError: RateLimitError: OpenAIException - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py:691\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[1;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[0;32m    690\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 691\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OpenAIError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py:619\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[1;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[0;32m    607\u001b[0m logging_obj\u001b[38;5;241m.\u001b[39mpre_call(\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    609\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mopenai_client\u001b[38;5;241m.\u001b[39mapi_key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    615\u001b[0m     },\n\u001b[0;32m    616\u001b[0m )\n\u001b[0;32m    618\u001b[0m headers, response \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 619\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_sync_openai_chat_completion_request(\n\u001b[0;32m    620\u001b[0m         openai_client\u001b[38;5;241m=\u001b[39mopenai_client,\n\u001b[0;32m    621\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    622\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    623\u001b[0m     )\n\u001b[0;32m    624\u001b[0m )\n\u001b[0;32m    626\u001b[0m logging_obj\u001b[38;5;241m.\u001b[39mmodel_call_details[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m headers\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py:439\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[1;34m(self, openai_client, data, timeout)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py:421\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[1;34m(self, openai_client, data, timeout)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 421\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m openai_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mwith_raw_response\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    422\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    423\u001b[0m     )\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_legacy_response.py:356\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    354\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[1;32m--> 356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:859\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    858\u001b[0m validate_response_format(response_format)\n\u001b[1;32m--> 859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    861\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    862\u001b[0m         {\n\u001b[0;32m    863\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    864\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    865\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m    866\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    867\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    868\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    869\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    870\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    871\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    872\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    873\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    874\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m    875\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    876\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    877\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m    878\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    879\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    880\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    881\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    882\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    883\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    884\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    885\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    886\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    887\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    888\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    889\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    890\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    891\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    892\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    893\u001b[0m         },\n\u001b[0;32m    894\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    895\u001b[0m     ),\n\u001b[0;32m    896\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    897\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    898\u001b[0m     ),\n\u001b[0;32m    899\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    900\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    901\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    902\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1280\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1281\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1282\u001b[0m )\n\u001b[1;32m-> 1283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    961\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    962\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    963\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    964\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    965\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m    966\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1048\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1050\u001b[0m         input_options,\n\u001b[0;32m   1051\u001b[0m         cast_to,\n\u001b[0;32m   1052\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1053\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1054\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1055\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1056\u001b[0m     )\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1098\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1096\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1099\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1100\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1101\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1102\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1103\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1104\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1048\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1050\u001b[0m         input_options,\n\u001b[0;32m   1051\u001b[0m         cast_to,\n\u001b[0;32m   1052\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1053\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1054\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1055\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1056\u001b[0m     )\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1098\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1096\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1099\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1100\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1101\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1102\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1103\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1104\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1073\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\litellm\\main.py:1610\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   1604\u001b[0m     logging\u001b[38;5;241m.\u001b[39mpost_call(\n\u001b[0;32m   1605\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m   1606\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[0;32m   1607\u001b[0m         original_response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m   1608\u001b[0m         additional_args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: headers},\n\u001b[0;32m   1609\u001b[0m     )\n\u001b[1;32m-> 1610\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optional_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1613\u001b[0m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\litellm\\main.py:1583\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   1582\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1583\u001b[0m     response \u001b[38;5;241m=\u001b[39m openai_chat_completions\u001b[38;5;241m.\u001b[39mcompletion(\n\u001b[0;32m   1584\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1585\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m   1586\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1587\u001b[0m         model_response\u001b[38;5;241m=\u001b[39mmodel_response,\n\u001b[0;32m   1588\u001b[0m         print_verbose\u001b[38;5;241m=\u001b[39mprint_verbose,\n\u001b[0;32m   1589\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[0;32m   1590\u001b[0m         api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[0;32m   1591\u001b[0m         acompletion\u001b[38;5;241m=\u001b[39macompletion,\n\u001b[0;32m   1592\u001b[0m         logging_obj\u001b[38;5;241m=\u001b[39mlogging,\n\u001b[0;32m   1593\u001b[0m         optional_params\u001b[38;5;241m=\u001b[39moptional_params,\n\u001b[0;32m   1594\u001b[0m         litellm_params\u001b[38;5;241m=\u001b[39mlitellm_params,\n\u001b[0;32m   1595\u001b[0m         logger_fn\u001b[38;5;241m=\u001b[39mlogger_fn,\n\u001b[0;32m   1596\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1597\u001b[0m         custom_prompt_dict\u001b[38;5;241m=\u001b[39mcustom_prompt_dict,\n\u001b[0;32m   1598\u001b[0m         client\u001b[38;5;241m=\u001b[39mclient,  \u001b[38;5;66;03m# pass AsyncOpenAI, OpenAI client\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m         organization\u001b[38;5;241m=\u001b[39morganization,\n\u001b[0;32m   1600\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[0;32m   1601\u001b[0m     )\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1603\u001b[0m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py:701\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[1;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[0;32m    700\u001b[0m     error_headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(error_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 701\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    702\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mstatus_code, message\u001b[38;5;241m=\u001b[39merror_text, headers\u001b[38;5;241m=\u001b[39merror_headers\n\u001b[0;32m    703\u001b[0m )\n",
      "\u001b[1;31mOpenAIError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m crew\u001b[38;5;241m.\u001b[39mkickoff(inputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArtificial Intelligence\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\crewai\\crew.py:548\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    545\u001b[0m metrics: List[UsageMetrics] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39msequential:\n\u001b[1;32m--> 548\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sequential_process()\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39mhierarchical:\n\u001b[0;32m    550\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_hierarchical_process()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\crewai\\crew.py:655\u001b[0m, in \u001b[0;36mCrew._run_sequential_process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CrewOutput:\n\u001b[0;32m    654\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_tasks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\crewai\\crew.py:756\u001b[0m, in \u001b[0;36mCrew._execute_tasks\u001b[1;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[0;32m    753\u001b[0m     futures\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m    755\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_context(task, task_outputs)\n\u001b[1;32m--> 756\u001b[0m task_output \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mexecute_sync(\n\u001b[0;32m    757\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent_to_use,\n\u001b[0;32m    758\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m    759\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools_for_task,\n\u001b[0;32m    760\u001b[0m )\n\u001b[0;32m    761\u001b[0m task_outputs \u001b[38;5;241m=\u001b[39m [task_output]\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_task_result(task, task_output)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\crewai\\task.py:302\u001b[0m, in \u001b[0;36mTask.execute_sync\u001b[1;34m(self, agent, context, tools)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_sync\u001b[39m(\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    297\u001b[0m     agent: Optional[BaseAgent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    298\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    299\u001b[0m     tools: Optional[List[BaseTool]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    300\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskOutput:\n\u001b[0;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_core(agent, context, tools)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\crewai\\task.py:366\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[1;34m(self, agent, context, tools)\u001b[0m\n\u001b[0;32m    362\u001b[0m tools \u001b[38;5;241m=\u001b[39m tools \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_by_agents\u001b[38;5;241m.\u001b[39madd(agent\u001b[38;5;241m.\u001b[39mrole)\n\u001b[1;32m--> 366\u001b[0m result \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mexecute_task(\n\u001b[0;32m    367\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    368\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m    369\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m    370\u001b[0m )\n\u001b[0;32m    372\u001b[0m pydantic_output, json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[0;32m    373\u001b[0m task_output \u001b[38;5;241m=\u001b[39m TaskOutput(\n\u001b[0;32m    374\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    375\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_format(),\n\u001b[0;32m    382\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\crewai\\agent.py:345\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[1;34m(self, task, context, tools)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retry_limit:\n\u001b[0;32m    344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 345\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_task(task, context, tools)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rpm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpm_controller:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpm_controller\u001b[38;5;241m.\u001b[39mstop_rpm_counter()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\crewai\\agent.py:345\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[1;34m(self, task, context, tools)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retry_limit:\n\u001b[0;32m    344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 345\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_task(task, context, tools)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rpm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpm_controller:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpm_controller\u001b[38;5;241m.\u001b[39mstop_rpm_counter()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\crewai\\agent.py:344\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[1;34m(self, task, context, tools)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retry_limit:\n\u001b[1;32m--> 344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    345\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_task(task, context, tools)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rpm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpm_controller:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\crewai\\agent.py:333\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[1;34m(self, task, context, tools)\u001b[0m\n\u001b[0;32m    330\u001b[0m     task_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_trained_data(task_prompt\u001b[38;5;241m=\u001b[39mtask_prompt)\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 333\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    334\u001b[0m         {\n\u001b[0;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: task_prompt,\n\u001b[0;32m    336\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_names\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39mtools_names,\n\u001b[0;32m    337\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39mtools_description,\n\u001b[0;32m    338\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: task\u001b[38;5;241m.\u001b[39mhuman_input,\n\u001b[0;32m    339\u001b[0m         }\n\u001b[0;32m    340\u001b[0m     )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:102\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_start_logs()\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m--> 102\u001b[0m formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_loop()\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input:\n\u001b[0;32m    105\u001b[0m     formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_human_feedback(formatted_answer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:206\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[1;34m(self, formatted_answer)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_loop(formatted_answer)\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_logs(formatted_answer)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_answer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:115\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[1;34m(self, formatted_answer)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentFinish):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_within_rpm_limit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_within_rpm_limit():\n\u001b[1;32m--> 115\u001b[0m         answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mcall(\n\u001b[0;32m    116\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages,\n\u001b[0;32m    117\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m    118\u001b[0m         )\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    121\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    122\u001b[0m                 content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived None or empty response from LLM call.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    123\u001b[0m                 color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    124\u001b[0m             )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\crewai\\llm.py:181\u001b[0m, in \u001b[0;36mLLM.call\u001b[1;34m(self, messages, callbacks)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;66;03m# Remove None values to avoid passing unnecessary parameters\u001b[39;00m\n\u001b[0;32m    179\u001b[0m     params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m--> 181\u001b[0m     response \u001b[38;5;241m=\u001b[39m litellm\u001b[38;5;241m.\u001b[39mcompletion(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\litellm\\utils.py:1029\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[0;32m   1026\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[0;32m   1027\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[0;32m   1028\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[1;32m-> 1029\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\litellm\\utils.py:905\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    903\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    904\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[1;32m--> 905\u001b[0m result \u001b[38;5;241m=\u001b[39m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    906\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\litellm\\main.py:2965\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   2962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m   2963\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2964\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[1;32m-> 2965\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[0;32m   2966\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   2967\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[0;32m   2968\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39me,\n\u001b[0;32m   2969\u001b[0m         completion_kwargs\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2970\u001b[0m         extra_kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   2971\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2189\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m   2187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[0;32m   2188\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[1;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   2190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2191\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:423\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[0;32m    422\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[0;32m    424\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    425\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    426\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[0;32m    427\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    428\u001b[0m         litellm_debug_info\u001b[38;5;241m=\u001b[39mextra_information,\n\u001b[0;32m    429\u001b[0m     )\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m503\u001b[39m:\n\u001b[0;32m    431\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: litellm.RateLimitError: RateLimitError: OpenAIException - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "result = crew.kickoff(inputs={\"topic\": \"Artificial Intelligence\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe9676-bccc-4a63-adba-0a1e90e1ea09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
